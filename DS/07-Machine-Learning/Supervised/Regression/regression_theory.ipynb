{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "- Linear Regression is a types of supervised machine learning where we attempt to predict continuous variables given several independent variables.\n",
    "\n",
    "- **Goal:**\n",
    "    - To create a mathematical model that captures linear relationship between dependent and independent variables.\n",
    "    - To generate predictions.\n",
    "    - To assist in Decision Making by quantifying the expected outcomes based on changes in independent/predictors variables.\n",
    "\n",
    "- \n",
    "- **Regression Types**:\n",
    "    1. `Simple Linear Regression`\n",
    "    2. `Multiple Linear Regresssion`\n",
    "    3. `Polynomial Regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Linear Regression\n",
    "\n",
    "- In Simple Linear Regression, we consider a single independent variable and a single dependent variable.\n",
    "- It helps to figure out the relationship between 2 variables i.e. independent variable (say x) and dependent variable (say y).\n",
    "- Mathematical model of simple linear regression takes the form of straight line.\n",
    "\n",
    "- **Mathematically,**\n",
    "    - `Y = β0 + β1X` \n",
    "        - β0: the intercept\n",
    "        - β1: the slope\n",
    "        - X: an independent variable (the variable used to predict Y)\n",
    "        - Y: dependent variable (the variable we want to predict)\n",
    "- Here the term β0, β1 are called model parameters which will be estimated using optimization techniques such as Gradient Descent via minimizing the objective or cost function.\n",
    "\n",
    "- **Visually (β0=38423, β1=821)**\n",
    "    - `Y = 38423 + 821X`\n",
    "    - <img src='images/1.png' width='400'>\n",
    "    - Here, we fit the Linear Regression Line on Scatter data.\n",
    "    - After, we can make prediction on New X via project to the line and looking to corresponding Y value\n",
    "\n",
    "- **Simple Linear Regression (Training Process Using Gradient Descent)**\n",
    "    1. `Initialization`\n",
    "    2. `Compute Cost`\n",
    "    3. `Gradient Descent Optimization`  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simple Linear Regression (Training Process Using Ordinary Least Squares (OLS))**\n",
    "- OLS stands for Ordinary Least Square.\n",
    "- OLS by default uses mean squared error.\n",
    "- Sum of Squared error i.e. SSE (β0 and β1)\n",
    "    - Σ(Yi - (β0 + β1 * Xi))² \n",
    "- Mean Squared Error  i.e. MSE (β0 and β1) \n",
    "    - SSE (β0 and β1) / N\n",
    "- OLS says using some formulae, It is possible to compute β0 and β1.\n",
    "- Aim: To select the best fit line (optimal β0 and β1) that reduces the error between predicted and actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of OLS Estimate for Single Linear Regression\n",
    "\n",
    "We start with the simple linear regression model:\n",
    "\n",
    "\n",
    "Y = β0 + β1*X\n",
    "\n",
    "\n",
    "Where:\n",
    "- Y is the observed dependent variable.\n",
    "- X  is the independent variable.\n",
    "- β0 is the intercept.\n",
    "- β1 is the slope.\n",
    "\n",
    "**Step 1: Define the Cost Function**\n",
    "\n",
    "The goal is to minimize the sum of squared errors (SSE), which is the sum of the squared differences between the observed Y and the predicted \n",
    "$(\\hat{Y})$ values:\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{N} (Y_i - \\hat{Y}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "**Step 2: Minimize SSE by Finding $(\\beta_0)$**\n",
    "\n",
    "To minimize SSE, we differentiate it with respect to $( \\beta_0 )$ and set the derivative equal to 0 (Since Minimum error is the point where Derivative of Error function is 0).\n",
    "\n",
    "Differentiate SSE with respect to $(\\beta_1)$: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta_0} = \\frac{\\partial \\left( \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))^2 \\right)}{\\partial \\beta_1}\n",
    "$$\n",
    "\n",
    "Apply the chain rule and simplify:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta_0} = -2 \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))\n",
    "$$\n",
    "\n",
    "Set the derivative equal to 0 and solve for $(\\beta_0)$:  \n",
    "$$\n",
    "-2 \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i)) = 0  \n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i)) = 0\n",
    "$$\n",
    "\n",
    "Now, solving for $(\\beta_0)$:  \n",
    "\n",
    "$$\n",
    "\\beta_0 = \\bar{Y} - \\beta_1\\bar{X}          --> (i)\n",
    "$$\n",
    "\n",
    "\n",
    "**Step 3: Minimize SSE by Finding $(\\beta_1)$**\n",
    "\n",
    "To minimize SSE, we differentiate it with respect to $( \\beta_1 )$ and set the derivative equal to 0 (Since Minimum error is the point where Derivative of Error function is 0).\n",
    "\n",
    "Differentiate SSE with respect to $(\\beta_1)$: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta_1} = \\frac{\\partial \\left( \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))^2 \\right)}{\\partial \\beta_1}\n",
    "$$\n",
    "\n",
    "Apply the chain rule and simplify:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta_1} = -2 \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))X_i\n",
    "$$\n",
    "\n",
    "Set the derivative equal to 0 and solve for $( \\beta_1 )$:\n",
    "\n",
    "$$\n",
    "-2 \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))X_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))X_i = 0\n",
    "$$\n",
    "\n",
    "Solving for $( \\beta_1)$ by substituting value of $( \\beta_0)$:\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum_{i=1}^{N} (Y_i - \\bar{Y})}{\\sum_{i=1}^{N} X_i - \\bar{X}} \n",
    "$$\n",
    "\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum_{i=1}^{N} (Y_i - \\bar{Y})}{\\sum_{i=1}^{N} X_i - \\bar{X}} \n",
    "$$  \n",
    "\n",
    "\n",
    "$$\n",
    "\\beta_0 = \\bar{Y} - \\beta_1\\bar{X}          \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression (using OLS)\n",
    "1. Define the Linear Model\n",
    "    - Y = β0 + β1*X  \n",
    "    \n",
    "    - Y is the observed dependent variable.\n",
    "    - X  is the independent variable.\n",
    "    - β0 is the intercept.\n",
    "    - β1 is the slope.\n",
    "\n",
    "2. Define the objective function\n",
    "    - Objective in Linear Regression is to find β0 and β1 that minimizes the sum of squared errors (SSE).\n",
    "    - The error for each data point is the difference between the observed value and the predicted value:\n",
    "    - `e(i) = Y(i) -  β0 + β1*X(i)`\n",
    "    - SSE = Σ(Yi - (β0 + β1 * Xi))²\n",
    "\n",
    "3. Minimize the Objective function\n",
    "    - As computed in the above section, To minimize SSE, we take the partial derivative of SSE with respect to β0 and β1, set them equal to zero, and solve for β0 and β1.\n",
    "    - Refer to above section for the derivation part.\n",
    "\n",
    "4. Final OLS Equation\n",
    "    - $\\hat{Y}$ = β0 + β1*X\n",
    "    - where, β0 and β1 is esimated using the formulae derived in the above section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simple Linear Regression (Training Process using Gradient Descent)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \n",
    "- https://www.youtube.com/watch?v=KZ1mWboXE6g\n",
    "- https://www.coursera.org/learn/data-analysis-with-python/lecture/Wlyce/linear-regression-and-multiple-linear-regression\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
