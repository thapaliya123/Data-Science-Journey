{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "- Clustering is an unsupervised machine learning technique that involves grouping similar data points together based on certain characteristics or features.\n",
    "- As mentioned clustering is an unsupervised machine learning, meaning it doesn't rely on labeled data but insted identifies and relationships within data itself.\n",
    "- Outcome of Clustering is `clusters`.\n",
    "    - Cluster is a group of objects that are similar to other objects in the cluster and dissimilar to data points in other cluster.\n",
    "- Unlike Classification, Clustering is an unsupervised learning algorithm, where the data is unlabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Application\n",
    "1. `Customer Segmentation`\n",
    "    - Clustering can be used to group customers with similar buying behaviors for targeted marketing.\n",
    "\n",
    "2. `Document Clustering`\n",
    "    - It's used in text analysis to group similar documents together.\n",
    "\n",
    "3. `Publication Media`\n",
    "    - Auto-categorizing news based on their content.\n",
    "    - Recommending similar news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Clustering Algorithms\n",
    "1. `partitioned-based clustering`\n",
    "    - Relatively efficient\n",
    "    - Clusters are of identical shape\n",
    "    - Useful for medium and large sized databases\n",
    "    - Example: K-Means, K-Medians, etc\n",
    "\n",
    "2. `Hierarchical Clustering`    \n",
    "    - produces trees of clusters\n",
    "    - very intuitive \n",
    "    - generally use with small size datasets\n",
    "\n",
    "3. `Density-based Clustering`\n",
    "    - Produces arbitrary shaped cluster.\n",
    "    - Useful when there is noise in your dataset\n",
    "    - E.g. DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Key Concepts\n",
    "- `Data points`\n",
    "    - They are sample or rows present in the dataset\n",
    "\n",
    "- `Clusters`\n",
    "    - Clusters are the output of the clustering process\n",
    "    - Cluster is a group of objects that are similar to other objects in the cluster and dissimilar to data points in other cluster.\n",
    "\n",
    "- `Centroid`\n",
    "    - In many clustering algorithms (e.g K-means), each cluster is represented by a central point called a centroid, which is the mean of all the data points in that cluster.\n",
    "\n",
    "- `Distance Metric`\n",
    "    - In an unsupervised setting, each samples in the datasets have no labels. So, How do you figure out similar and dissimilar samples?\n",
    "    - The answer is using Distance Metric.\n",
    "    - Common distance metrics include Euclidean distance, Manhattan Distance, and cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering\n",
    "- KMeans is a popular clustering algorithms, which attempts to partition data into K clusters, where K is a user-defined parameter.\n",
    "- `Objective:`\n",
    "    - To form cluster in such a way that similar samples falls into same cluster and dissimilar samples falls into different clusters i.e.\n",
    "        - Examples within a cluster are very similar.\n",
    "        - Examples across different cluster are very different.\n",
    "- `How to figure out whether two data points are similar or dissimilar?`\n",
    "    - Using Distance metrics\n",
    "    - Different Distance Metrics can be Euclidean distance, Manhattan Distance, Cosine Similarity\n",
    "    - In KMeans we use Euclidean distance i.e.\n",
    "    - $Euclidean Distance = \\sqrt x^2 + y^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Algorithm\n",
    "1. `Inititialize K centroids`  \n",
    "    - There are various initialization methods like randomly selecting K data points as centroids or using advanced techniques like k-means++.  \n",
    "2. `Calculate the distance of each data point from each centroid.`\n",
    "    - Generally, Euclidean distance is used. However, we can also use other distance metrics as well.\n",
    "3. `Assign each data point (object) to its closest centroid, creating a cluster.`\n",
    "4. `Recalculate the positions of the K centroids.`\n",
    "    - New centroids are calculated by taking mean of all data points associated to each cluster.\n",
    "5. `Repeat the steps 2-4 until the centroids no longer move.`\n",
    "    - i.e. Repeat process, until two steps has a same centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "- We discussed 2 different techniques to initialize Cluster Centroids. Now let's discuss each of them.\n",
    "    1. Random Initialization\n",
    "        - In this technique, the initial cluster centroids are chosen randomly from the data points.\n",
    "        - It's a simple and quick method, but it can lead to suboptimal results since the initial centroids may be far from the actual clusters.\n",
    "\n",
    "    2. K-means++ Initialization  \n",
    "        - 2.1 Select the First Centroid\n",
    "            - Randomly select one data point from the dataset as the first centroid without any specific criteria\n",
    "\n",
    "        - 2.2 Calculate Distances\n",
    "            - For each data point in the dataset, calculate the squared distance between that data point and the nearest centroid that has already been choosen. \n",
    "            - Distance Metric can be: Euclidean Distance.\n",
    "        \n",
    "        - 2.3 Choose Subsequent Centroids\n",
    "            -  Choose the next centroid from the remaining data points with a probability proportional to its squared distance to the nearest existing centroid. This ensures that data points farther away from existing centroids are more likely to be selected as new centroids.  \n",
    "        \n",
    "        - 2.4 Repeat steps 2.2 and 2.3 until K centroids are selected.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose Optimal K"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
