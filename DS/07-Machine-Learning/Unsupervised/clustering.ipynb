{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "- Clustering is an unsupervised machine learning technique that involves grouping similar data points together based on certain characteristics or features.\n",
    "- As mentioned clustering is an unsupervised machine learning, meaning it doesn't rely on labeled data but insted identifies and relationships within data itself.\n",
    "- Outcome of Clustering is `clusters`.\n",
    "    - Cluster is a group of objects that are similar to other objects in the cluster and dissimilar to data points in other cluster.\n",
    "\n",
    "- <img src='images/2.png' width='500'>  \n",
    "\n",
    "    - [Image Source](https://analystprep.com/study-notes/wp-content/uploads/2021/03/cfa-level-2-classification-vs-clustering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Application\n",
    "1. `Customer Segmentation`\n",
    "    - Clustering can be used to group customers with similar buying behaviors for targeted marketing.\n",
    "\n",
    "2. `Document Clustering`\n",
    "    - It's used in text analysis to group similar documents together.\n",
    "\n",
    "3. `Publication Media`\n",
    "    - Auto-categorizing news based on their content.\n",
    "    - Recommending similar news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Clustering Algorithms\n",
    "1. `partitioned-based clustering`\n",
    "    - Relatively efficient\n",
    "    - Clusters are of identical shape\n",
    "    - Useful for medium and large sized databases\n",
    "    - Example: K-Means, K-Medians, etc\n",
    "\n",
    "2. `Hierarchical Clustering`    \n",
    "    - produces trees of clusters\n",
    "    - very intuitive \n",
    "    - generally use with small size datasets\n",
    "\n",
    "3. `Density-based Clustering`\n",
    "    - Produces arbitrary shaped cluster.\n",
    "    - Useful when there is noise in your dataset\n",
    "    - E.g. DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering\n",
    "- KMeans is a popular clustering algorithms, which attempts to partition data into K clusters, where K is a user-defined parameter.\n",
    "- `Objective:`\n",
    "    - To form cluster in such a way that similar samples falls into same cluster and dissimilar samples falls into different clusters i.e.\n",
    "        - Examples within a cluster are very similar.\n",
    "        - Examples across different cluster are very different.\n",
    "- `How to figure out whether two data points are similar or dissimilar?`\n",
    "    - Using Distance metrics\n",
    "    - Different Distance Metrics can be Euclidean distance, Manhattan Distance, Cosine Similarity\n",
    "    - In KMeans we use Euclidean distance (2D) i.e.\n",
    "     $$Euclidean Distance = \\sqrt{{(x_2 - x_1)^2 + (y_2 - y_1)^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Algorithm\n",
    "1. `Inititialize K centroids`  \n",
    "    - There are various initialization methods like randomly selecting K data points as centroids or using advanced techniques like k-means++.  \n",
    "2. `Calculate the distance of each data point from each centroid.`\n",
    "    - Generally, Euclidean distance is used. However, we can also use other distance metrics as well.\n",
    "3. `Assign each data point (object) to its closest centroid, creating a cluster.`\n",
    "4. `Recalculate the positions of the K centroids.`\n",
    "    - New centroids are calculated by taking mean of all data points associated to each cluster.\n",
    "5. `Repeat the steps 2-4 until the centroids no longer move.`\n",
    "    - i.e. Repeat process, until two steps has a same centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose Optimal K\n",
    "- The first step in KMeans clustering is to choose K. So What is the best value of K?\n",
    "- In order to determine the optimal K value, we have 2 popular methods i.e. `Elbow Method` and `Silhouette Analysis`\n",
    "- `1. Elbow Method`\n",
    "    - The Elbow Method is a graphical appraoch that involves running the K-means algorithm for a range of values of K and plotting the cost (inertia) of the clusters as a function of K.\n",
    "    - When we increase number of clusters the average distance of centroid to data points will always reduces. This means increasing K will always decreases error.\n",
    "    - Hence, Elbow point is the point where the rate of metric decreases sharply when increasing K as shown in figure below:\n",
    "    - <img src='images/1.png' width='500'>\n",
    "                    \n",
    "        - [Image Source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Felbow-method-vs-silhouette-co-efficient-in-determining-the-number-of-clusters-33baff2fbeee&psig=AOvVaw0U--DloH3XgFr9eWLkqx4M&ust=1696244161193000&source=images&cd=vfe&opi=89978449&ved=0CBEQjRxqFwoTCIDArofY1IEDFQAAAAAdAAAAABAT)\n",
    "    -\n",
    "    - **`Steps:`** \n",
    "        1. Run the KMeans algorithm for a range of values (e.g. K from 1 to maximum value).\n",
    "        2. For each K, Calculate the Within sum of squared distances (inertia) between data points and their assigned cluster centroids.\n",
    "        3. Plot the K values against their respective inertia values.\n",
    "        4. Look for the \"elbow\" point on the plot, where the inertia starts to decrease at a slower rate. This point is the optimal K.\n",
    "\n",
    "- `2. Silhouette Analysis`\n",
    "    - It measures how similar an object to its own cluster (cohesion) compared to other clusters (separation).\n",
    "    - Range: [-1, 1]\n",
    "    - Higher values means, object is well matched to its own cluster and poorly matched to neighboring clusters.   \n",
    "    - **Steps:**\n",
    "        - Run the K-means algorithm for different values of K.\n",
    "        - For each K, calculate the silhouette score for each data point using formula:\n",
    "            $$\n",
    "            S(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
    "            $$   \n",
    "\n",
    "            Where:  \n",
    "                S(i) is the silhouette score for data point \"i.\"   \n",
    "                a(i) is the average distance from data point \"i\" to the other data points within the same cluster (intra-cluster distance).  \n",
    "                b(i) is the average distance from data point \"i\" to data points in any other cluster, except its own (inter-cluster distance). \n",
    "\n",
    "        - Compute the average silhouette coefficient for each clustering result.\n",
    "        - Plot the number of clusters against the corresponding average silhouette coefficient.\n",
    "        - Look for the highest average silhouette coefficient, indicating the number of clusters that best separates the data. \n",
    "\n",
    "    - The silhouette score ranges from -1 to 1, where high value indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "- Once we have clustered our data points, How do we determine the goodness of cluster?\n",
    "- Hence, Evaluation Metrics helps us to decide how good our Cluster is is.\n",
    "- Common evaluation metrics are:\n",
    "    1. `WCSS (Within-Cluster Sum of Squares)`\n",
    "        - It measures the compactness or cohesion of clusters. It quantifies the total variance within all clusters.\n",
    "        - WCSS is calculated as the sum of the squared distances between each data point in a cluster and its centroid.\n",
    "        - Mathematically, for each cluster \"C\", it is calculated as:\n",
    "        \n",
    "            $$\n",
    "            WCSS(C) = \\sum_{i \\in C} \\left\\| x_i - \\text{centroid}(C) \\right\\|^2\n",
    "            $$\n",
    "\n",
    "            Where:\n",
    "            - $WCSS(C)$ represents the WCSS for cluster $(C)$.\n",
    "            - $(\\sum_{i \\in C})$ represents the sum over all data points \\(i\\) in cluster \\(C\\).\n",
    "            - $(\\left\\| x_i - \\text{centroid}(C) \\right\\|^2)$ calculates the squared distance between each data point \\(x_i\\) in cluster \\(C\\) and the centroid of cluster \\(C\\).  \n",
    "\n",
    "    2. `Silhouette Score`\n",
    "        - It is a measure of how similar an object is to its own cluster (cohesion) compared to other cluster (separation).\n",
    "        - It's fall falls in the range -1 to 1, high value indicate good clusters.\n",
    "        - The detailed steps in computing silhouette score is provided in above sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
