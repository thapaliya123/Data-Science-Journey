{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- Data preprocessing is an essential step in building a machine learning model.\n",
    "- Final result from the trained machine learning model will depend on how well the data has been preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing in NLP\n",
    "- Text preprocessing is the first step in the process of building a machine learning model.\n",
    "- The text is represented as a vector in multi-dimensional space.\n",
    "- Various text preprocessing steps are:\n",
    "    - Tokenization\n",
    "    - Lower casing\n",
    "    - Stop words removal\n",
    "    - Stemming \n",
    "    - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment below cell to install nltk\n",
    "\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/fm-pc-\n",
      "[nltk_data]     lt-125/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Basics\n",
    "- NLTK stands for `Natural Language Toolkit`\n",
    "- We will use nltk library to preprocess the textual data.\n",
    "- In order to work with NLTK we need to first install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test string\n",
    "\n",
    "PHRASE = \"\"\"Data preprocessing is the essential steps in building a machine learnig model. Final result from the trained machine learning model\n",
    "    will depend on how well the data has been preprocessed. NLTK is the library that helps us to preprocess the textual data and various other\n",
    "    textual analysis.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "- Tokenization is the process of splitting bigger chunks of text into smaller chunks.\n",
    "- Mainly there are 3 types of Tokenization:\n",
    "    - `Character Tokenization`\n",
    "        - Character Tokenization is the process of splitting text chunks into character level.\n",
    "    - `Word Tokenization`\n",
    "        - Word Tokenization is the process of splitting sentences into words.\n",
    "        - Words, numbers, punctuations are also called as tokens in NLP.\n",
    "     \n",
    "    - `Sentence Tokenization`\n",
    "        - Sentence Tokenization is the process of splitting phrases or paragraphs into Sentences.\n",
    "        - A sentence usually ends by a full stop, so splitting strings with character '.' also a process of sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'a', 't', 'a', ' ', 'p', 'r', 'e', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'e', 's', 's', 'e', 'n', 't', 'i', 'a', 'l', ' ', 's', 't', 'e', 'p', 's', ' ', 'i', 'n', ' ', 'b', 'u', 'i', 'l', 'd', 'i', 'n', 'g', ' ', 'a', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'g', ' ', 'm', 'o', 'd', 'e', 'l', '.', ' ', 'F', 'i', 'n', 'a', 'l', ' ', 'r', 'e', 's', 'u', 'l', 't', ' ', 'f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'e', 'd', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'm', 'o', 'd', 'e', 'l', '\\n', ' ', ' ', ' ', ' ', 'w', 'i', 'l', 'l', ' ', 'd', 'e', 'p', 'e', 'n', 'd', ' ', 'o', 'n', ' ', 'h', 'o', 'w', ' ', 'w', 'e', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'd', 'a', 't', 'a', ' ', 'h', 'a', 's', ' ', 'b', 'e', 'e', 'n', ' ', 'p', 'r', 'e', 'p', 'r', 'o', 'c', 'e', 's', 's', 'e', 'd', '.', ' ', 'N', 'L', 'T', 'K', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'y', ' ', 't', 'h', 'a', 't', ' ', 'h', 'e', 'l', 'p', 's', ' ', 'u', 's', ' ', 't', 'o', ' ', 'p', 'r', 'e', 'p', 'r', 'o', 'c', 'e', 's', 's', ' ', 't', 'h', 'e', ' ', 't', 'e', 'x', 't', 'u', 'a', 'l', ' ', 'd', 'a', 't', 'a', ' ', 'a', 'n', 'd', ' ', 'v', 'a', 'r', 'i', 'o', 'u', 's', ' ', 'o', 't', 'h', 'e', 'r', '\\n', ' ', ' ', ' ', ' ', 't', 'e', 'x', 't', 'u', 'a', 'l', ' ', 'a', 'n', 'a', 'l', 'y', 's', 'i', 's', '.', '\\n', ' ', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "# character tokenization\n",
    "characters = list(PHRASE)\n",
    "\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'preprocessing', 'is', 'the', 'essential', 'steps', 'in', 'building', 'a', 'machine', 'learnig', 'model', '.', 'Final', 'result', 'from', 'the', 'trained', 'machine', 'learning', 'model', 'will', 'depend', 'on', 'how', 'well', 'the', 'data', 'has', 'been', 'preprocessed', '.', 'NLTK', 'is', 'the', 'library', 'that', 'helps', 'us', 'to', 'preprocess', 'the', 'textual', 'data', 'and', 'various', 'other', 'textual', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# word tokenization\n",
    "words = word_tokenize(PHRASE)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data preprocessing is the essential steps in building a machine learnig model.', 'Final result from the trained machine learning model\\n    will depend on how well the data has been preprocessed.', 'NLTK is the library that helps us to preprocess the textual data and various other\\n    textual analysis.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# sentence tokenization\n",
    "sentences = sent_tokenize(PHRASE)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lower Casing\n",
    "- Convert words to lower casing\n",
    "- **Why?**\n",
    "    - Words like `Lower` and `lower` means the same,\n",
    "    - No use of lower case means similar words as discussed above will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing is the essential steps in building a machine learnig model. final result from the trained machine learning model\n",
      "    will depend on how well the data has been preprocessed. nltk is the library that helps us to preprocess the textual data and various other\n",
      "    textual analysis.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# lower casing\n",
    "lower_phrase = PHRASE.lower()\n",
    "\n",
    "print(lower_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stopwords Removal\n",
    "- In NLP, useless or repeated words (data) are referred to as `stopwords`.\n",
    "- Stopwords are very commonly used words in the documents like `a, an, the, is, are, etc`.\n",
    "- These kind of words do not signify any importance as they do not help in distinguishing two documents.\n",
    "- NLTK in python has a list of stopwords stored in 16 different languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'preprocessing', 'essential', 'steps', 'building', 'machine', 'learnig', 'model', '.', 'Final', 'result', 'trained', 'machine', 'learning', 'model', 'depend', 'well', 'data', 'preprocessed', '.', 'NLTK', 'library', 'helps', 'us', 'preprocess', 'textual', 'data', 'various', 'textual', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# stopwords removal\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "no_stop_words_list = [word for word in words if word.lower() not in stopwords_list]\n",
    "\n",
    "print(no_stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stemming\n",
    "- Stemming is a process of transforming a word to its root form.\n",
    "- Stemming reduces the words \"chocolates\", \"chocolatey\", \"choco\" to the root word \"chocolate\"\n",
    "- Stemming is an important part in the pipelining process in Natural Language Processing.\n",
    "- Stemming is useful when you do not care much of contextual information, Since words obtained after stemming may or may not have actual Dictionary meaning.\n",
    "- **Example:**\n",
    "    - likes, liked, likely, liking --> like\n",
    "    - history, historical --> histori\n",
    "    - finally, final, finalized --> fina\n",
    "    - going, goes --> go\n",
    "- **Errors in Stemming:**\n",
    "    1. `Over Stemming`\n",
    "        - It is the process where a much larger part of a word is chopped off than what is required, which in turn leads to two or more words being reduced to the same root word or stem incorrectly when they should have been reduced to two or more stem words.\n",
    "        - Example: Unversity and universe reduced to same word \"univers\"\n",
    "    2. `Under Stemming`\n",
    "        - In under stemming, two or more words could be wrongly reduced to more than one root word, when they actually should be reduced to the same root word.\n",
    "        - Example: when words \"data\" and \"datum\" reduced to word dat and datu\n",
    "- **Cons:**\n",
    "    - Obtained stemmed words may not have actual Dictionary meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'preprocess', 'essenti', 'step', 'build', 'machin', 'learnig', 'model', '.', 'final', 'result', 'train', 'machin', 'learn', 'model', 'depend', 'well', 'data', 'preprocess', '.', 'nltk', 'librari', 'help', 'us', 'preprocess', 'textual', 'data', 'variou', 'textual', 'analysi', '.']\n"
     ]
    }
   ],
   "source": [
    "## stemming words\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_phrase = [ps.stem(word) for word in no_stop_words_list]\n",
    "\n",
    "print(stemmed_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Lemmatization\n",
    "- Unlike stemming, lemmatization reduces the words to a word existing in the Dictionary.\n",
    "- Libraries such as nltk, have stemmers and lemmatizers implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization of word **bats** is: bat\n",
      "Lemmatization of word **are** is: are\n",
      "Lemmatization of word **feet** is: foot\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization: WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize single word\n",
    "print(\"Lemmatization of word **bats** is:\", lemmatizer.lemmatize('bats'))\n",
    "print(\"Lemmatization of word **are** is:\", lemmatizer.lemmatize('are'))\n",
    "print(\"Lemmatization of word **feet** is:\", lemmatizer.lemmatize('feet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'preprocessing', 'essential', 'step', 'building', 'machine', 'learnig', 'model', '.', 'Final', 'result', 'trained', 'machine', 'learning', 'model', 'depend', 'well', 'data', 'preprocessed', '.', 'NLTK', 'library', 'help', 'u', 'preprocess', 'textual', 'data', 'various', 'textual', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize sentence\n",
    "\n",
    "lemmatized_phrase = [lemmatizer.lemmatize(word) for word in no_stop_words_list]\n",
    "print(lemmatized_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better accuracy we need to pass pos tag associated with each words in a sentence. This is because meaning of words changes based on the context in which it may arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'preprocessing', 'essential', 'step', 'building', 'machine', 'learnig', 'model', '.', 'Final', 'result', 'train', 'machine', 'learn', 'model', 'depend', 'well', 'data', 'preprocessed', '.', 'NLTK', 'library', 'help', 'u', 'preprocess', 'textual', 'data', 'various', 'textual', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper() # E.g. [('Data', 'NNS')] --> N\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# get lemmas\n",
    "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in no_stop_words_list]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Stemming and Lemmatization\n",
    "\n",
    "| Aspect                  | Stemming                                  | Lemmatization                        |\n",
    "|-------------------------|------------------------------------------|-------------------------------------|\n",
    "| Context Consideration   | Does not consider context; removes suffixes | Considers context; finds meaningful base forms |\n",
    "| Meaningful Base Forms   | May not result in meaningful base words  | Always results in meaningful base words |\n",
    "| Widely Used             | Widely used and implemented in multiple languages | Less commonly used due to complexity |\n",
    "| Ease of Implementation  | Relatively easy to implement and build custom stemmers | Complex, requires linguistic knowledge |\n",
    "| Example                 | \"Caring\" -> \"Car\"                         | \"Caring\" -> \"Care\"                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
